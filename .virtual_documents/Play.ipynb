import os
print(os.getcwd())
#os.chdir("./LLM")
#print(os.getcwd())



from LLM.LLMSupport import *
import pathlib
import os
from BDDTestingLLM_args import parse_args
logging.getLogger().setLevel(logging.INFO)



args = parse_args(with_json_args=pathlib.Path(os.environ["LLM_PARAMS_PATH_INFERENCE"]))
cg = BDDTestingLLM(args)
cg.prepare_inference(push_to_hub=False)


cg.prepare_data()


answer = cg.train_dataset[0]['text']

response_template = " ### Answer:"
response_idx = answer.find(response_template)
input_prompt = answer[:response_idx + len(response_template)]

input_ids = cg.tokenizer(input_prompt, return_tensors="pt").to("cuda")['input_ids'][0]
#print(input_ids)
print(len(input_ids))
#print(input_prompt)

# Next, create a chat and apply the chat template
messages = [
  {"role": "system", "content": "You are a bot that responds to weather queries."},
  {"role": "user", "content": "Hey, what's the temperature in Paris now?"}
]

messages[-1]["content"] = input_prompt 
outputs = cg.pipeline(
    messages,
    max_new_tokens=4096,
)
print(outputs[0]["generated_text"][-1])


cg.do_inference(input_prompt, max_generated_tokens=4096, store_history=False)


cg.do_inference("What parameters are in the following behave library given step?: @given(\"the car has (?P<engine_power>\d+) kw, weighs (?P<weight>\d+) kg, has a drag coefficient of (?P<drag>[\.\d]+)\")")


cg.train_data
