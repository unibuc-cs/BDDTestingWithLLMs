{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29744400-5793-46f5-b69e-efabcdcfe8e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T07:40:56.677366Z",
     "start_time": "2024-09-08T07:40:56.673479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/cipri/OneDrive - University of Bucharest, Faculty of Mathematics and Computer Science/WORK_PROGRESS/BDD and LLM/Our\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# print(os.getcwd())\n",
    "# #os.chdir(\"./LLM\")\n",
    "# #print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cce576a-9b88-48ef-a9ed-595b3430a4ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T07:40:39.328755Z",
     "start_time": "2024-09-08T07:40:36.925274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114c97f10b744a88b02176133307ffcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d334241d7ec249a58d8caeb5e4372147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.08480376642881861\n",
      "Model: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pprint import pprint\n",
    "from LLM.LLMSupport import AgentTypePrompt, LLMConvHistory\n",
    "import pathlib\n",
    "from typing import List, Dict, Any, Union, Tuple, Optional, Sequence\n",
    "import os\n",
    "#from BDDTestingLLM_args import parse_args\n",
    "from BDDToLLM import *\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "#args = parse_args(with_json_args=pathlib.Path(os.environ[\"LLM_PARAMS_PATH_INFERENCE\"]))\n",
    "bdd = BDDGenerationWithLLM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40bb91c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Model inference - Prompt as text:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Environment: ipython\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a BDD testing expert who can write scenarios using the Gherkin language, Python and the behave library.If possible, use the tools provided first. \n",
      "If no function is suitable for the user query, answer without tools, as if you were the expert.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Given the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n",
      "\n",
      "Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.Do not use variables.\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"global_tool_add_new_empty_step\",\n",
      "        \"description\": \"Add a new empty step to the step implementation file. The clause is the step to be added and the filepath is the path to the step implementation file. The Filepath should be a path to a Python file.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"clause\": {\n",
      "                    \"type\": \"string\",\n",
      "                    \"description\": \"The clause to be added as a new step.\"\n",
      "                },\n",
      "                \"filepath\": {\n",
      "                    \"type\": \"string\",\n",
      "                    \"description\": \"The path to the step implementation file\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\n",
      "                \"clause\",\n",
      "                \"filepath\"\n",
      "            ]\n",
      "        },\n",
      "        \"return\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"A string containing the test case.\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"global_tool_clear_current_test\",\n",
      "        \"description\": \"Clears, discards the current test in progress and removes the file from disk if it was saved previously.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {}\n",
      "        },\n",
      "        \"return\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"A string containing the test case.\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"global_tool_find_path_from_user\",\n",
      "        \"description\": \"Given the last user message, extract a path from the user message that corresponds to the path of a file where the test will be saved.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {}\n",
      "        },\n",
      "        \"return\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"A string containing the test case.\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"global_tool_find_path_lists_from_user\",\n",
      "        \"description\": \"Given the last user message, extract a list of paths from the user message.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {}\n",
      "        },\n",
      "        \"return\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"A string containing the test case.\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"global_tool_get_current_test\",\n",
      "        \"description\": \"Get and/or shows the current test as a string in the format of a Behave test case.\\nDoes not save the test\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {}\n",
      "        },\n",
      "        \"return\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"A string containing the test case.\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"global_tool_run_test\",\n",
      "        \"description\": \"Runs,  execute or give a try to see the output of the current test using pytest or behave.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {}\n",
      "        },\n",
      "        \"return\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"A string containing the test case.\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "{\n",
      "    \"type\": \"function\",\n",
      "    \"function\": {\n",
      "        \"name\": \"global_tool_save_current_test\",\n",
      "        \"description\": \"Saves the current test to a file that was specified by the user in the conversation.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {}\n",
      "        },\n",
      "        \"return\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"A string containing the test case.\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "Show the current test<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "### Model inference - Outputs:\n",
      " [{'generated_text': '{\"name\": \"global_tool_get_current_test\", \"parameters\": {}}'}]\n",
      "### Functions called by the assistant: [{'name': 'global_tool_get_current_test', 'arguments': {}}]\n",
      "### Raw assistent output:\n",
      " {\"name\": \"global_tool_get_current_test\", \"parameters\": {}}\n",
      "Tools called: [{'name': 'global_tool_get_current_test', 'arguments': {}}]\n"
     ]
    }
   ],
   "source": [
    "from BDDTestingLLM_functions import BDDAssistant, BASE_SYSTEM_MESSAGE, BASE_SYSTEM_MESSAGE_WITH_TOOLS_CALL #, hook_set_global_conversation_context #, hook_execute_tool_use_operation, hook_get_all_tool_use_operations, hook_set_global_conversation_context\n",
    "\n",
    "bdd.llm.full_conv_history.reset()\n",
    "convManager = BDDAssistant(bdd)\n",
    "\n",
    "    \n",
    "#print(BDDAssistant.instance)\n",
    "#BDDAssistant.execute_tool_use_operation(tool_use_op=\"global_tool_get_current_test\")\n",
    "\n",
    "#convManager.do_inference(\"Show the current test\", store_history=True, execute_tools=True)\n",
    " \n",
    "# Case 1: General conversation with tools call\n",
    "if True:\n",
    "    convManager.do_inference(\"Show the current test\", \n",
    "                            store_history=True, \n",
    "                                agent_mode=AgentTypePrompt.AGENT_TYPE_PROMPT_TOOL_CALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4d8d3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Model inference - Prompt as text:\n",
      " <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a BDD testing expert who can write scenarios using the Gherkin language, Python and the behave library.If possible, use the tools provided first. \n",
      "If no function is suitable for the user query, answer without tools, as if you were the expert.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Show the current test<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"name\": \"global_tool_get_current_test\", \"parameters\": {}}<|eot_id|><|start_header_id|>ipython<|end_header_id|>\n",
      "\n",
      "ToolUseOpResult(success=True, message=Scenario Outline: \n",
      ")<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell me a joke<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "### Model inference - Outputs:\n",
      " [{'generated_text': \"Here's a simple BDD test scenario for a joke:\\n\\n```gherkin\\nFeature: Joke Teller\\n  As a user\\n  I want to get a joke\\n  So that I can laugh\\n\\n  Scenario: Get a joke\\n    Given I am on the joke page\\n    When I request a joke\\n    Then I should see a joke\\n```\\n\\nAnd here's the Python code to implement this scenario using the behave library:\\n\\n```python\\nfrom behave import given, when, then\\n\\n@given('I am on the joke page')\\ndef step_impl(context):\\n    # Implement the logic to navigate to the joke page\\n    pass\\n\\n@when('I request a joke')\\ndef step_impl(context):\\n    # Implement the logic to request a joke\\n    pass\\n\\n@then('I should see a joke')\\ndef step_impl(context):\\n    # Implement the logic to verify the joke is displayed\\n    pass\\n```\\n\\nThis is a very basic example and would need to be expanded upon to actually implement the joke telling functionality.\"}]\n",
      "### Raw assistent output:\n",
      " Here's a simple BDD test scenario for a joke:\n",
      "\n",
      "```gherkin\n",
      "Feature: Joke Teller\n",
      "  As a user\n",
      "  I want to get a joke\n",
      "  So that I can laugh\n",
      "\n",
      "  Scenario: Get a joke\n",
      "    Given I am on the joke page\n",
      "    When I request a joke\n",
      "    Then I should see a joke\n",
      "```\n",
      "\n",
      "And here's the Python code to implement this scenario using the behave library:\n",
      "\n",
      "```python\n",
      "from behave import given, when, then\n",
      "\n",
      "@given('I am on the joke page')\n",
      "def step_impl(context):\n",
      "    # Implement the logic to navigate to the joke page\n",
      "    pass\n",
      "\n",
      "@when('I request a joke')\n",
      "def step_impl(context):\n",
      "    # Implement the logic to request a joke\n",
      "    pass\n",
      "\n",
      "@then('I should see a joke')\n",
      "def step_impl(context):\n",
      "    # Implement the logic to verify the joke is displayed\n",
      "    pass\n",
      "```\n",
      "\n",
      "This is a very basic example and would need to be expanded upon to actually implement the joke telling functionality.\n",
      "Tools called: []\n",
      "[{'content': 'You are a BDD testing expert who can write scenarios using the '\n",
      "             'Gherkin language, Python and the behave library.If possible, use '\n",
      "             'the tools provided first. \\n'\n",
      "             'If no function is suitable for the user query, answer without '\n",
      "             'tools, as if you were the expert. ',\n",
      "  'role': 'system'},\n",
      " {'content': 'Show the current test', 'role': 'user'},\n",
      " {'role': 'assistant',\n",
      "  'tool_calls': [{'function': {'arguments': {},\n",
      "                               'name': 'global_tool_get_current_test'},\n",
      "                  'type': 'function'}]},\n",
      " {'content': ToolUseOpResult(success=True, message=Scenario Outline: \n",
      "),\n",
      "  'name': 'global_tool_get_current_test',\n",
      "  'role': 'tool'},\n",
      " {'content': 'Tell me a joke', 'role': 'user'},\n",
      " {'content': \"Here's a simple BDD test scenario for a joke:\\n\"\n",
      "             '\\n'\n",
      "             '```gherkin\\n'\n",
      "             'Feature: Joke Teller\\n'\n",
      "             '  As a user\\n'\n",
      "             '  I want to get a joke\\n'\n",
      "             '  So that I can laugh\\n'\n",
      "             '\\n'\n",
      "             '  Scenario: Get a joke\\n'\n",
      "             '    Given I am on the joke page\\n'\n",
      "             '    When I request a joke\\n'\n",
      "             '    Then I should see a joke\\n'\n",
      "             '```\\n'\n",
      "             '\\n'\n",
      "             \"And here's the Python code to implement this scenario using the \"\n",
      "             'behave library:\\n'\n",
      "             '\\n'\n",
      "             '```python\\n'\n",
      "             'from behave import given, when, then\\n'\n",
      "             '\\n'\n",
      "             \"@given('I am on the joke page')\\n\"\n",
      "             'def step_impl(context):\\n'\n",
      "             '    # Implement the logic to navigate to the joke page\\n'\n",
      "             '    pass\\n'\n",
      "             '\\n'\n",
      "             \"@when('I request a joke')\\n\"\n",
      "             'def step_impl(context):\\n'\n",
      "             '    # Implement the logic to request a joke\\n'\n",
      "             '    pass\\n'\n",
      "             '\\n'\n",
      "             \"@then('I should see a joke')\\n\"\n",
      "             'def step_impl(context):\\n'\n",
      "             '    # Implement the logic to verify the joke is displayed\\n'\n",
      "             '    pass\\n'\n",
      "             '```\\n'\n",
      "             '\\n'\n",
      "             'This is a very basic example and would need to be expanded upon '\n",
      "             'to actually implement the joke telling functionality.',\n",
      "  'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "# Case 2: General conversation without tools call\n",
    "if True:\n",
    "    temp_general_conv_hist = LLMConvHistory()\n",
    "    temp_general_conv_hist.set_base_system_prompt(BASE_SYSTEM_MESSAGE)\n",
    "    convManager.do_inference(\"Tell me a joke\", \n",
    "                            store_history=True, \n",
    "                            agent_mode=AgentTypePrompt.AGENT_TYPE_PROMPT_GENERAL_CONV)\n",
    "\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(convManager.get_conv_hist().get_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d32b67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convManager' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#temp_prompt = temp_prompt.format() #(user_question=\"Tell me a joke\")\u001b[39;00m\n\u001b[1;32m     25\u001b[0m agent_match_system_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCheck the user question against the functions description above. \u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124mRespond with a JSON format as below.\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124mIf no match is found, respond with the following JSON format:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m}\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 40\u001b[0m convManager\u001b[38;5;241m.\u001b[39minit_conv_history(override_base_system_message\u001b[38;5;241m=\u001b[39magent_match_system_message)\n\u001b[1;32m     43\u001b[0m conv_history_overridden \u001b[38;5;241m=\u001b[39m LLMConvHistory(base_system_prompt\u001b[38;5;241m=\u001b[39magent_match_system_message)\n\u001b[1;32m     45\u001b[0m convManager\u001b[38;5;241m.\u001b[39mdo_inference(temp_prompt, \n\u001b[1;32m     46\u001b[0m                          store_history\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,                         \n\u001b[1;32m     47\u001b[0m                          max_generated_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     48\u001b[0m                          agent_mode \u001b[38;5;241m=\u001b[39m AgentTypePrompt\u001b[38;5;241m.\u001b[39mAGENT_TYPE_PROMPT_TOOL_MATCH,\n\u001b[1;32m     49\u001b[0m                          override_conv_history\u001b[38;5;241m=\u001b[39mconv_history_overridden,\n\u001b[1;32m     50\u001b[0m                          )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convManager' is not defined"
     ]
    }
   ],
   "source": [
    "from LLM.LLMSupport import AgentTypePrompt, LLMConvHistory\n",
    "\n",
    "\n",
    "temp_prompt = \"\"\"Given the functions below, is there any function description usable for the user question?\n",
    " \n",
    "### Functions:\n",
    "{\n",
    "    name: \"global_tool_get_current_test\", \n",
    "    description: \"Get and/or shows the current test as a string in the format of a Behave test case.\\nDoes not save the test\"\n",
    "},\n",
    "\n",
    "{\n",
    "    name: \"global_tool_get_all_tool_use_operations\", \n",
    "    description: \"Get all tool use operations available in the system.\\nDoes not save the test\"\n",
    "}\n",
    "\n",
    "### User Question: \"\"\" \n",
    "\n",
    "\n",
    "user_question = \"Tell me a joke\"\n",
    "\n",
    "temp_prompt += user_question\n",
    " \n",
    "#temp_prompt = temp_prompt.format() #(user_question=\"Tell me a joke\")\n",
    "agent_match_system_message = \"\"\"Check the user question against the functions description above. \n",
    "Respond with a JSON format as below.\n",
    "If no match is found, respond with the following JSON format:\n",
    "{\n",
    "    \"answer\": NO_MATCH,\n",
    "    \"matched_function\": \"\"\n",
    "}\n",
    "\n",
    "If found, say the function name as below:\n",
    "{\n",
    "    \"answer\": MATCH,\n",
    "    \"matched_function\": \"name of the function\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "convManager.init_conv_history(override_base_system_message=agent_match_system_message)\n",
    "\n",
    "\n",
    "conv_history_overridden = LLMConvHistory(base_system_prompt=agent_match_system_message)\n",
    "\n",
    "convManager.do_inference(temp_prompt, \n",
    "                         store_history=True,                         \n",
    "                         max_generated_tokens=1024,\n",
    "                         agent_mode = AgentTypePrompt.AGENT_TYPE_PROMPT_TOOL_MATCH,\n",
    "                         override_conv_history=conv_history_overridden,\n",
    "                         )\n",
    "\n",
    "pprint(convManager.get_conv_hist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hook_set_global_conversation_context(convManager)\n",
    "\n",
    "tool_name_to_function = hook_get_all_tool_use_operations() \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"### Error in command_test_from_user: {e}\")\n",
    "        convManager.pop_current_conversation()\n",
    "        raise e         \n",
    "    finally:\n",
    "        #convManager.set_conv_hist(initial_conv)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "#command_test_from_user(\"Save the current test to the provided path\", \"tool_save_current_test\")\n",
    "command_test_from_user(\"Save the current test \", \"global_tool_save_current_test\")\n",
    "command_test_from_user(\"Show the current test \", \"global_tool_get_current_test\")\n",
    "command_test_from_user(\"Discard the in-progress test \", \"global_tool_clear_current_test\")\n",
    "command_test_from_user(\"Execute the in-progress test \", \"global_tool_run_test\")\n",
    "#test_find_path_lists_from_user()\n",
    "#test_find_path_from_user()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e574f83-7dd4-44ce-a92a-a8c3b629d3b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T07:24:39.473068Z",
     "start_time": "2024-09-08T07:24:39.466022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cffdea259e354bccb215ca1524754b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70c009a1d694e69b8ce0763f9f159b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The validation split percentage 5 or dataset size 1 is too low. Please increase it. For now the script will use the entire dataset for training and validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05578f8775a542b788439ce5cdd3195f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running a formatting operation on the dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88144ac144174fd5943ce5e04db3f2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running a formatting operation on the dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cg.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1efb57fe-5982-4d32-a1c5-751b9e29a048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "{'role': 'assistant', 'content': '{\"name\": \"get_current_temperature\", \"parameters\": {\"location\": \"Paris, France\"}}'}\n"
     ]
    }
   ],
   "source": [
    "answer = cg.train_dataset[0]['text']\n",
    "\n",
    "response_template = \" ### Answer:\"\n",
    "response_idx = answer.find(response_template)\n",
    "input_prompt = answer[:response_idx + len(response_template)]\n",
    "\n",
    "input_ids = cg.tokenizer(input_prompt, return_tensors=\"pt\").to(\"cuda\")['input_ids'][0]\n",
    "#print(input_ids)\n",
    "print(len(input_ids))\n",
    "#print(input_prompt)\n",
    "\n",
    "# Next, create a chat and apply the chat template\n",
    "messages = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a bot that responds to weather queries.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Hey, what's the temperature in Paris now?\"}\n",
    "]\n",
    "\n",
    "messages[-1][\"content\"] = input_prompt \n",
    "outputs = cg.pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c909bfe4-e280-4255-95d9-25818a267722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T07:23:50.738455Z",
     "start_time": "2024-09-08T07:23:50.730613Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '{\"name\": \"get_current_temperature\", \"parameters\": {\"location\": \"Paris, France\"}}'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg.do_inference(input_prompt, max_generated_tokens=4096, store_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f77e1dd-5e3d-40e2-a9b3-bd3b238a34b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T07:24:07.472777Z",
     "start_time": "2024-09-08T07:24:07.465803Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cg\u001b[38;5;241m.\u001b[39mdo_inference(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat parameters are in the following behave library given step?: @given(\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mthe car has (?P<engine_power>\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+) kw, weighs (?P<weight>\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+) kg, has a drag coefficient of (?P<drag>[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md]+)\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cg' is not defined"
     ]
    }
   ],
   "source": [
    "cg.do_inference(\"What parameters are in the following behave library given step?: @given(\\\"the car has (?P<engine_power>\\d+) kw, weighs (?P<weight>\\d+) kg, has a drag coefficient of (?P<drag>[\\.\\d]+)\\\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b44b528fb382950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T07:24:49.176389Z",
     "start_time": "2024-09-08T07:24:49.169243Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cg\u001b[38;5;241m.\u001b[39mtrain_data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cg' is not defined"
     ]
    }
   ],
   "source": [
    "cg.train_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
