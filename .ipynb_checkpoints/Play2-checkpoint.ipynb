{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e368d42b-9d9b-4425-adcf-1e9c624cc89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6e5b2f1d0f434898052164fcbcd11e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from pprint import pprint\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a BDD testing expert that can write scenarios using Gherkin language, Python language and behave library\"},\n",
    "    {\"role\": \"user\", \"content\": \"Show me a scenario for testing Mario like games\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "def inference(prompt: str, max_tokens: int ) -> str:\n",
    "    messages[-1][\"content\"] = prompt\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=max_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    return outputs[0][\"generated_text\"][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec7006c0-1b06-4d0a-8a65-d96007b045a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Plain result: {\\n'\n",
      " '  \"found\": true,\\n'\n",
      " '  \"step_found\":  \"the car has (?P<engine_power>\\\\d+) kw, weighs '\n",
      " '(?P<weight>\\\\d+) kg, has a drag coefficient of (?P<drag>[\\\\.\\\\d]+)\"\\n'\n",
      " '}')\n",
      "(\"The model didn't find any match. The raw output is {\\n\"\n",
      " '     \"engine_power\" : \"124 kw\",\\n'\n",
      " '     \"weight\" : \"12345 kg\",\\n'\n",
      " '     \"drag\" : \"123\"\\n'\n",
      " '}')\n",
      "('The model matched the step: False\\n'\n",
      " \"             Response: The model didn't find any match. The raw output is {\\n\"\n",
      " '     \"engine_power\" : \"124 kw\",\\n'\n",
      " '     \"weight\" : \"12345 kg\",\\n'\n",
      " '     \"drag\" : \"123\"\\n'\n",
      " '}\\n')\n"
     ]
    }
   ],
   "source": [
    "USE_DEBUG = False \n",
    "from typing import Union, Tuple\n",
    "\n",
    "# Takes an input step in natural language and tries to match against a set of available input steps in a Gherkin file.\n",
    "# Two steps are used in the process:\n",
    "    # Step 1: try to find the closest in terms of matching\n",
    "    # Step 2: try to match parameters. Report the error if not succeeded\n",
    "def _match_input_step_to_set(self, \n",
    "                            USER_INPUT_STEP: str, \n",
    "                            USER_INPUT_AVAILABLE_STEPS: str,\n",
    "                            max_generated_tokens : int = 1024) -> Tuple[bool, str]:\n",
    "    \n",
    "    match_rewrite_prompt_template= \"\"\"Given the below Gherkin available steps, check if any can match the input step. \n",
    "    \n",
    "    Use Json syntax for response. Use the following format if any step can be matched:\n",
    "    {{\n",
    "      \"found\": true,\n",
    "      \"step_found\":  \"the step you found closest\"\n",
    "    }}\n",
    "    \n",
    "    If no available option is OK, then use:\n",
    "    {{\n",
    "        \"found\": false,\n",
    "    }}\n",
    "    \n",
    "    Do not provide any other information or examples.\n",
    "    \n",
    "    ### Input step: \n",
    "    {user_input_step_to_match}\n",
    "    \n",
    "    ### Available steps:\n",
    "    {user_input_available_steps}\"\"\"\n",
    "    \n",
    "    prompt_matching_params_template = \"\"\"Can you match the parameters in the input text step with the target step ?\n",
    "    \n",
    "    Example:\n",
    "    ### Input step: The plane has a travel speed of 123 km/h and a lenght of 500 m\n",
    "    ### Output step: @given(A plane that has a (?P<speed>\\d+) km/h, length (?P<size>\\d+) m\n",
    "    Response:\n",
    "    {{\n",
    "     \"speed\" : \"123 km/h\",\n",
    "     \"size\" : \"500 m\"\n",
    "    }}\n",
    "    \n",
    "    Question:\n",
    "    ### Input step: {user_input_step}\n",
    "    ### Output step: {step_str}\n",
    "    Response:    your response \n",
    "    \n",
    "    Do not write anything else.\n",
    "    \"\"\"\n",
    "    \n",
    "    match_rewrite_prompt_template.format(user_input_step_to_match=USER_INPUT_STEP, \n",
    "                                         user_input_available_steps=USER_INPUT_AVAILABLE_STEPS)\n",
    "    \n",
    "    import json \n",
    "    res = inference(match_rewrite_prompt, max_generated_tokens)[\"content\"]\n",
    "    pprint(f\"Plain result: {res}\")\n",
    "    \n",
    "    # Loading in json\n",
    "    res_json = None\n",
    "    try:\n",
    "        dir = json.loads(res2)\n",
    "        res_json = dir\n",
    "    except Exception as e:\n",
    "        pprint(f\"exception occured while reading the output: {e}\")\n",
    "    \n",
    "    \n",
    "    step_found_str = res_json.get(\"step_found\", None)\n",
    "    \n",
    "    if step_found_str:\n",
    "        prompt_matching_params = prompt_matching_params_template.format(user_input_step=USER_INPUT_STEP, step_str=step_found_str)\n",
    "        res22 = inference(prompt_matching_params, max_generated_tokens)[\"content\"]\n",
    "        \n",
    "        RESPONSE_TAG = \"Response:\"\n",
    "        resp_json_begin_index = res22.find(RESPONSE_TAG)\n",
    "        resp_json_end_index = res22.rfind(\"}\")\n",
    "        \n",
    "        if resp_json_begin_index !=-1 and resp_json_end_index!= -1:\n",
    "            resp_json_str = res22[resp_json_begin_index + len(RESPONSE_TAG) : resp_json_end_index + 1]\n",
    "            \n",
    "            try:\n",
    "                resp_json = json.loads(resp_json_str)\n",
    "                pprint(resp_json)\n",
    "\n",
    "                return (True, resp_json)\n",
    "            except Exception as e:\n",
    "                msg = f\"Error {e} when parsing for parameters:\\n{resp_json_str}\"\n",
    "                pprint(msg)\n",
    "                \n",
    "                return (False, msg)\n",
    "    \n",
    "    \n",
    "    msg = \"The model didn't find any match. The raw output is {temp_resp}\".format(temp_resp=res22)\n",
    "    pprint(msg)\n",
    "    return (False, msg)\n",
    "\n",
    "def match_input_step_to_set(self, \n",
    "                            USER_INPUT_STEP: str, \n",
    "                            max_generated_tokens : int = 1024) -> Tuple[bool, str]:\n",
    "    \n",
    "    # TODO: take from file with source code \n",
    "    USER_INPUT_AVAILABLE_STEPS = \"\"\"@given(\"the car has (?P<engine_power>\\d+) kw, weighs (?P<weight>\\d+) kg, has a drag coefficient of (?P<drag>[\\.\\d]+)\")\n",
    "    \n",
    "    @given(\"a frontal area of (?P<area>.+) m\\^2\")\n",
    "    \n",
    "    @when(\"I accelerate to (?P<speed>\\d+) km/h\")\n",
    "    \n",
    "    @then(\"the time should be within (?P<precision>[\\d\\.]+)s of (?P<time>[\\d\\.]+)s\")\n",
    "    \n",
    "    @given(\"that the car is moving at (?P<speed>\\d+) m/s\")\n",
    "    \n",
    "    @when(\"I brake at (?P<brake_force>\\d+)% force\")\n",
    "    \n",
    "    @step(\"(?P<seconds>\\d+) seconds? pass(?:es)?\")\n",
    "    \n",
    "    @then(\"I should have traveled less than (?P<distance>\\d+) meters\")\n",
    "    \n",
    "    @given(\"that the car's heading is (?P<heading>\\d+) deg\")\n",
    "    \n",
    "    @when(\"I turn (?P<direction>left|right) at a yaw rate of (?P<rate>\\d+) deg/sec for (?P<duration>\\d+) seconds\")\n",
    "    \n",
    "    @then(\"the car's heading should be (?P<heading>\\d+) deg\")\"\"\"\n",
    "    \n",
    "    is_matched, resp = _match_input_step_to_set(self, USER_INPUT_STEP, USER_INPUT_AVAILABLE_STEPS, max_generated_tokens=1024)\n",
    "\n",
    "    return is_matched, resp \n",
    "\n",
    "\n",
    "is_matched, resp = match_input_step_to_set(None, USER_INPUT_STEP = \"A drag of 123, a mass of 12345 kg, and an engine of 124kw the Yoda's vehicle has!\")\n",
    "pprint(f\"The model matched the step: {is_matched}\\n \\\n",
    "            Response: {resp}\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
