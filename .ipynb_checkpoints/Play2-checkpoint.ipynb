{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42769706c937109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637f53302b69488b9025208adc98c670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"I am a Behavior-Driven Development (BDD) test writer. My primary role is to write automated tests in a natural language style, focusing on the behavior of the system under test. I use tools like Cucumber or SpecFlow to write tests in a language that's easy for both developers and non-technical stakeholders to understand.\\n\\nMy main goal is to ensure that the system meets the requirements and behaves as expected, while also providing a clear and concise description of the expected behavior. I work closely with developers, product owners, and other stakeholders to write tests that cover the desired functionality and edge cases.\\n\\nIn BDD, I typically write tests in the following format:\\n\\n* Given (preconditions)\\n* When (action or event)\\n* Then (expected outcome)\\n\\nFor example:\\n\\n```\\nFeature: User login\\n  As a user\\n  I want to login to the system\\n  So that I can access my account\\n\\nScenario: Successful login\\n  Given I have a valid username and password\\n  When I enter the username and password and click login\\n  Then I should be redirected to the dashboard\\n```\\n\\nThis format allows me to write tests that are easy to read and understand, while also providing a clear description of the expected behavior.\"}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a BDD test writer\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a128f97d-2352-444e-b972-b92458a96574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here\\'s a Python script that groups the paths into a JSON file as requested:\\n\\n```python\\nimport json\\nimport os\\n\\ndef group_paths(paths):\\n    \"\"\"\\n    Groups the paths into a JSON file.\\n    \\n    Args:\\n        paths (list): A list of paths.\\n    \\n    Returns:\\n        dict: A dictionary containing the grouped paths.\\n    \"\"\"\\n    grouped_paths = {\\n        \"paths\": paths\\n    }\\n    return grouped_paths\\n\\ndef main():\\n    # Define the paths\\n    paths = [\\n        \"./Data/behave\",\\n        \"/usr/bin\",\\n        \"/home/ciprian/paths\"\\n    ]\\n    \\n    # Group the paths\\n    grouped_paths = group_paths(paths)\\n    \\n    # Write the grouped paths to a JSON file\\n    with open(\\'paths.json\\', \\'w\\') as f:\\n        json.dump(grouped_paths, f, indent=4)\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\nWhen you run this script, it will create a JSON file named `paths.json` in the same directory as the script with the following content:\\n\\n```json\\n{\\n    \"paths\": [\\n        \"./Data/behave\",\\n        \"/usr/bin\",\\n        \"/home/ciprian/paths\"\\n    ]\\n}\\n```\\n\\nIf you want to get the paths from the user, you can modify the `main` function like this:\\n\\n```python\\ndef main():\\n    # Get the paths from the user\\n    paths = input(\"Enter the paths separated by commas: \")\\n    paths = [p.strip() for p in paths.split(\\',\\')]\\n    \\n    # Group the paths\\n    grouped_paths = group_paths(paths)\\n    \\n    # Write the grouped paths to a JSON file\\n    with open(\\'paths.json\\', \\'w\\') as f:\\n        json.dump(grouped_paths, f, indent=4)\\n```\\n\\nYou can then run the script and enter the paths when prompted. The script will then write the grouped paths to the `paths.json` file.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict \n",
    "\n",
    "messages = messages[:2]\n",
    "messages[-1][\"content\"] = \"\"\"The paths to code follows:\n",
    "                                \"./Data/behave\",\n",
    "                                \"/usr/bin\",\n",
    "                                \"/home/ciprian/paths\"\n",
    "                                \"\"\"\n",
    "def tool_find_path_from_userstr(self, llm_pipe, conv_hist: List[Dict]):\n",
    "\n",
    "    # Temorarly add the prompt request to extract path\n",
    "    conv_hist.append({\"role\": \"user\", \"content\": \"\"\"Group the files given by the last user message in a JSON file as bellow. \n",
    "    Do not write any code or variables, just extract the paths and fill the JSON below.\n",
    "\n",
    "        {\n",
    "            \"paths\" : [\"path1\", \"path2\", \"path3\", ...]\n",
    "        }\n",
    "\n",
    "        If you do not find any folder path from the user message, provide a list, as below, do not invent one.\n",
    "        {\n",
    "            \"paths\" : []\n",
    "        }\n",
    "        \"\"\"})\n",
    "\n",
    "    outputs = llm_pipe(\n",
    "        messages,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "\n",
    "    # Remove the last message from the conv hist\n",
    "    conv_hist = conv_hist[:-1]\n",
    "\n",
    "    last_gen_msg = outputs[0][\"generated_text\"][-1]\n",
    "    res = last_gen_msg[\"content\"] \n",
    "    return res\n",
    "\n",
    "self = None\n",
    "tool_find_path_from_userstr(self, pipeline, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72d67255c2d67f0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T07:49:19.667349Z",
     "start_time": "2024-09-06T07:49:19.664839Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain_openai import ChatOpenAI\n",
    "LLAMA_API = \"LA-a2ce4f869d2d48099a6135d60330a4c216e9504626e2492c9da30d762f16af1c\"\n",
    "LANGSMITH_API = \"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\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = LANGSMITH_API\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"SOME NAME\"\n",
    "\n",
    "\n",
    "# model = ChatOpenAI(\n",
    "#     openai_api_key=LLAMA_API,\n",
    "#     openai_api_base=\"https://api.llama-api.com\",\n",
    "#     model=\"llama3-70b\"\n",
    "# )\n",
    "\n",
    "model = pipeline.model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bbf226e7004d21f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T07:49:28.635453Z",
     "start_time": "2024-09-06T07:49:28.192458Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"telco.csv\")\n",
    "\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "\n",
    "customer_df = df[['Customer_ID', 'Gender', 'Age', 'Under_30', 'Senior_Citizen', 'Married', 'Dependents', 'Number_of_Dependents', 'Country', 'State', 'City', 'Zip_Code', 'Latitude', 'Longitude', 'Population']]\n",
    "service_df = df[['Customer_ID', 'Phone_Service', 'Multiple_Lines', 'Internet_Service', 'Internet_Type', 'Online_Security', 'Online_Backup', 'Device_Protection_Plan', 'Premium_Tech_Support', 'Streaming_TV', 'Streaming_Movies', 'Streaming_Music', 'Unlimited_Data']]\n",
    "billing_df = df[['Customer_ID', 'Tenure_in_Months', 'Offer', 'Avg_Monthly_Long_Distance_Charges', 'Avg_Monthly_GB_Download', 'Contract', 'Paperless_Billing', 'Payment_Method', 'Monthly_Charge', 'Total_Charges', 'Total_Refunds', 'Total_Extra_Data_Charges', 'Total_Long_Distance_Charges', 'Total_Revenue']]\n",
    "referral_df = df[['Customer_ID', 'Referred_a_Friend', 'Number_of_Referrals']]\n",
    "churn_df = df[['Customer_ID', 'Quarter', 'Satisfaction_Score', 'Customer_Status', 'Churn_Label', 'Churn_Score', 'CLTV', 'Churn_Category', 'Churn_Reason']]\n",
    "\n",
    "conn = sqlite3.connect('telco.db')\n",
    "\n",
    "customer_df.to_sql('Customer', conn, if_exists='replace', index=False)\n",
    "service_df.to_sql('Service', conn, if_exists='replace', index=False)\n",
    "billing_df.to_sql('Billing', conn, if_exists='replace', index=False)\n",
    "referral_df.to_sql('Referral', conn, if_exists='replace', index=False)\n",
    "churn_df.to_sql('Churn', conn, if_exists='replace', index=False)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2a2643054056160b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T07:49:28.638358Z",
     "start_time": "2024-09-06T07:49:28.636439Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_db(query):\n",
    "  conn = sqlite3.connect('telco.db')\n",
    "  try:\n",
    "    return pd.read_sql_query(query, conn)\n",
    "  finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6edcc13243f3230f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T07:49:28.640399Z",
     "start_time": "2024-09-06T07:49:28.638996Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fa97891265e3e9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T07:49:28.752641Z",
     "start_time": "2024-09-06T07:49:28.749831Z"
    }
   },
   "outputs": [],
   "source": [
    "DB_DESCRIPTION = \"\"\"You have access to the following tables and columns in a sqllite3 database:\n",
    "\n",
    "Customer Table\n",
    "Customer_ID: A unique ID that identifies each customer.\n",
    "Gender: The customer’s gender: Male, Female.\n",
    "Age: The customer’s current age, in years, at the time the fiscal quarter ended.\n",
    "Under_30: Indicates if the customer is under 30: Yes, No.\n",
    "Senior_Citizen: Indicates if the customer is 65 or older: Yes, No.\n",
    "Married: Indicates if the customer is married: Yes, No.\n",
    "Dependents: Indicates if the customer lives with any dependents: Yes, No.\n",
    "Number_of_Dependents: Indicates the number of dependents that live with the customer.\n",
    "Country: The country of the customer’s primary residence. Example: United States.\n",
    "State: The state of the customer’s primary residence.\n",
    "City: The city of the customer’s primary residence.\n",
    "Zip_Code: The zip code of the customer’s primary residence.\n",
    "Latitude: The latitude of the customer’s primary residence.\n",
    "Longitude: The longitude of the customer’s primary residence.\n",
    "Population: A current population estimate for the entire Zip Code area.\n",
    "\n",
    "Service Table\n",
    "Customer_ID: A unique ID that identifies each customer (Foreign Key).\n",
    "Phone_Service: Indicates if the customer subscribes to home phone service with the company: Yes, No.\n",
    "Multiple_Lines: Indicates if the customer subscribes to multiple telephone lines with the company: Yes, No.\n",
    "Internet_Service: Indicates if the customer subscribes to Internet service with the company: Yes, No.\n",
    "Internet_Type: Indicates the type of Internet service: DSL, Fiber Optic, Cable, None.\n",
    "Online_Security: Indicates if the customer subscribes to an additional online security service provided by the company: Yes, No.\n",
    "Online_Backup: Indicates if the customer subscribes to an additional online backup service provided by the company: Yes, No.\n",
    "Device_Protection Plan: Indicates if the customer subscribes to an additional device protection plan for their Internet equipment provided by the company: Yes, No.\n",
    "Premium_Tech_Support: Indicates if the customer subscribes to an additional technical support plan from the company with reduced wait times: Yes, No.\n",
    "Streaming_TV: Indicates if the customer uses their Internet service to stream television programming from a third party provider: Yes, No.\n",
    "Streaming_Movies: Indicates if the customer uses their Internet service to stream movies from a third party provider: Yes, No.\n",
    "Streaming_Music: Indicates if the customer uses their Internet service to stream music from a third party provider: Yes, No.\n",
    "Unlimited_Data: Indicates if the customer has paid an additional monthly fee to have unlimited data downloads/uploads: Yes, No.\n",
    "\n",
    "Billing Table\n",
    "Customer_ID: A unique ID that identifies each customer (Foreign Key).\n",
    "Tenure_in_Months: Indicates the total amount of months that the customer has been with the company by the end of the quarter specified above.\n",
    "Offer: Identifies the last marketing offer that the customer accepted, if applicable. Values include None, Offer A, Offer B, Offer C, Offer D, and Offer E.\n",
    "Avg_Monthly_Long_Distance_Charges: Indicates the customer’s average long distance charges, calculated to the end of the quarter specified above.\n",
    "Avg_Monthly_GB_Download: Indicates the customer’s average download volume in gigabytes, calculated to the end of the quarter specified above.\n",
    "Contract: Indicates the customer’s current contract type: Month-to-Month, One Year, Two Year.\n",
    "Paperless_Billing: Indicates if the customer has chosen paperless billing: Yes, No.\n",
    "Payment_Method: Indicates how the customer pays their bill: Bank Withdrawal, Credit Card, Mailed Check.\n",
    "Monthly_Charge: Indicates the customer’s current total monthly charge for all their services from the company.\n",
    "Total_Charges: Indicates the customer’s total charges, calculated to the end of the quarter specified above.\n",
    "Total_Refunds: Indicates the customer’s total refunds, calculated to the end of the quarter specified above.\n",
    "Total_Extra_Data_Charges: Indicates the customer’s total charges for extra data downloads above those specified in their plan, by the end of the quarter specified above.\n",
    "Total_Long_Distance_Charges: Indicates the customer’s total charges for long distance above those specified in their plan, by the end of the quarter specified above.\n",
    "Total_Revenue: The total revenue generated from the customer.\n",
    "\n",
    "Referral Table\n",
    "Customer_ID: A unique ID that identifies each customer (Foreign Key).\n",
    "Referred_a_Friend: Indicates if the customer has ever referred a friend or family member to this company: Yes, No.\n",
    "Number_of_Referrals: Indicates the number of referrals to date that the customer has made.\n",
    "\n",
    "Churn Table\n",
    "Customer_ID: A unique ID that identifies each customer (Foreign Key).\n",
    "Quarter: The fiscal quarter that the data has been derived from (e.g. Q3).\n",
    "Satisfaction_Score: A customer’s overall satisfaction rating of the company from 1 (Very Unsatisfied) to 5 (Very Satisfied).\n",
    "Customer_Status: Indicates the status of the customer at the end of the quarter: Churned, Stayed, Joined.\n",
    "Churn_Label: Yes = the customer left the company this quarter. No = the customer remained with the company.\n",
    "Churn_Score: A value from 0-100 that is calculated using the predictive tool IBM SPSS Modeler. The model incorporates multiple factors known to cause churn. The higher the score, the more likely the customer will churn.\n",
    "CLTV: Customer Lifetime Value. A predicted CLTV is calculated using corporate formulas and existing data. The higher the value, the more valuable the customer. High value customers should be monitored for churn.\n",
    "Churn_Category: A high-level category for the customer’s reason for churning: Attitude, Competitor, Dissatisfaction, Other, Price.\n",
    "Churn_Reason: A customer’s specific reason for leaving the company. Directly related to Churn Category.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2259155145f05a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T09:19:35.666694Z",
     "start_time": "2024-09-06T09:19:35.612098Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"StringPromptValue\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 60\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m#messages[0][\"content\"] = messages[0][\"content\"].format(data_description=DB_DESCRIPTION) \u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m#messages[1][\"content\"] = messages[1][\"content\"].format(question=\"Count customers by zip code. Return the 5 most common zip codes\")\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     can_answer_router \u001b[38;5;241m=\u001b[39m can_answer_router_prompt \u001b[38;5;241m|\u001b[39m pipeline \u001b[38;5;241m|\u001b[39m JsonOutputParser()\n\u001b[0;32m---> 60\u001b[0m     can_answer_router\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount customers by zip code. Return the 5 most common zip codes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_description\u001b[39m\u001b[38;5;124m\"\u001b[39m: DB_DESCRIPTION})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:2878\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2876\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2877\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2878\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:4474\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4460\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[1;32m   4461\u001b[0m \n\u001b[1;32m   4462\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4471\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[1;32m   4472\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m   4475\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke,\n\u001b[1;32m   4476\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4477\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc),\n\u001b[1;32m   4478\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4479\u001b[0m     )\n\u001b[1;32m   4480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4484\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:1785\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1782\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1783\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1784\u001b[0m         Output,\n\u001b[0;32m-> 1785\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m   1786\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m             config,\n\u001b[1;32m   1790\u001b[0m             run_manager,\n\u001b[1;32m   1791\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1792\u001b[0m         ),\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1795\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/config.py:398\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    397\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:4330\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4328\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   4329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4330\u001b[0m     output \u001b[38;5;241m=\u001b[39m call_func_with_variable_args(\n\u001b[1;32m   4331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   4332\u001b[0m     )\n\u001b[1;32m   4333\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[1;32m   4334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/config.py:398\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    397\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:262\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1257\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1251\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         )\n\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1263\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1263\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[1;32m   1264\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1265\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:294\u001b[0m, in \u001b[0;36mTextGenerationPipeline.preprocess\u001b[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, **generate_kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m    287\u001b[0m         prompt_text\u001b[38;5;241m.\u001b[39mmessages,\n\u001b[1;32m    288\u001b[0m         add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs,\n\u001b[1;32m    292\u001b[0m     )\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(prefix \u001b[38;5;241m+\u001b[39m prompt_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n\u001b[1;32m    296\u001b[0m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt_text\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle_long_generation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"StringPromptValue\") to str"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "can_answer_router_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a database reading bot that can answer users' questions using information from a database. \\n\n",
    "\n",
    "    {data_description} \\n\\n\n",
    "\n",
    "    Given the user's question, decide whether the question can be answered using the information in the database. \\n\\n\n",
    "\n",
    "    Return a JSON with two keys, 'reasoning' and 'can_answer', and no preamble or explanation.\n",
    "    Return one of the following JSON:\n",
    "    \n",
    "    {{\"reasoning\": \"I can find the average revenue of customers with tenure over 24 months by averaging the Total Revenue column in the Billing table filtered by Tenure in Months > 24\", \"can_answer\":true}}\n",
    "    {{\"reasoning\": \"I can find customers who signed up during the last 12 month using the Tenure in Months column in the Billing table\", \"can_answer\":true}}\n",
    "    {{\"reasoning\": \"I can't answer how many customers churned last year because the Churn table doesn't contain a year\", \"can_answer\":false}}\n",
    "    \n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"data_description\", \"question\"],\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \n",
    "        \"\"\"You are a database reading bot that can answer users' questions using information from a database. \\n {data_description} \\n\\n\n",
    "    \n",
    "        Given the user's question, decide whether the question can be answered using the information in the database. \\n\\n\n",
    "    \n",
    "        Return a JSON with two keys, 'reasoning' and 'can_answer', and no preamble or explanation.\n",
    "        Return one of the following JSON:\n",
    "        \n",
    "        {{\"reasoning\": \"I can find the average revenue of customers with tenure over 24 months by averaging the Total Revenue column in the Billing table filtered by Tenure in Months > 24\", \"can_answer\":true}}\n",
    "        {{\"reasoning\": \"I can find customers who signed up during the last 12 month using the Tenure in Months column in the Billing table\", \"can_answer\":true}}\n",
    "        {{\"reasoning\": \"I can't answer how many customers churned last year because the Churn table doesn't contain a year\", \"can_answer\":false}}\n",
    "    \n",
    "        \"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"Question: {question} \\n\"},\n",
    "]\n",
    "\n",
    "##### \n",
    "temp = 0\n",
    "if temp:\n",
    "    messages[0][\"content\"] = messages[0][\"content\"].format(data_description=DB_DESCRIPTION) \n",
    "    messages[1][\"content\"] = messages[1][\"content\"].format(question=\"Count customers by zip code. Return the 5 most common zip codes\")\n",
    "    \n",
    "    outputs = pipeline(messages, max_new_tokens=1024)\n",
    "    print(outputs[0][\"generated_text\"][-1])\n",
    "    \n",
    "    \n",
    "else:\n",
    "    #messages[0][\"content\"] = messages[0][\"content\"].format(data_description=DB_DESCRIPTION) \n",
    "    #messages[1][\"content\"] = messages[1][\"content\"].format(question=\"Count customers by zip code. Return the 5 most common zip codes\")\n",
    "    \n",
    "    can_answer_router = can_answer_router_prompt | pipeline | JsonOutputParser()\n",
    "    can_answer_router.invoke({\"question\": \"Count customers by zip code. Return the 5 most common zip codes\", \"data_description\": DB_DESCRIPTION})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1819c253f839284a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T07:49:29.134152Z",
     "start_time": "2024-09-06T07:49:29.128958Z"
    }
   },
   "outputs": [],
   "source": [
    "can_answer_router_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a database reading bot that can answer users' questions using information from a database. \\n\n",
    "\n",
    "    {data_description} \\n\\n\n",
    "\n",
    "    Given the user's question, decide whether the question can be answered using the information in the database. \\n\\n\n",
    "\n",
    "    Return a JSON with two keys, 'reasoning' and 'can_answer', and no preamble or explanation.\n",
    "    Return one of the following JSON:\n",
    "    \n",
    "    {{\"reasoning\": \"I can find the average revenue of customers with tenure over 24 months by averaging the Total Revenue column in the Billing table filtered by Tenure in Months > 24\", \"can_answer\":true}}\n",
    "    {{\"reasoning\": \"I can find customers who signed up during the last 12 month using the Tenure in Months column in the Billing table\", \"can_answer\":true}}\n",
    "    {{\"reasoning\": \"I can't answer how many customers churned last year because the Churn table doesn't contain a year\", \"can_answer\":false}}\n",
    "    \n",
    "\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"data_description\", \"question\"],\n",
    ")\n",
    "\n",
    "can_answer_router = can_answer_router_prompt | model | JsonOutputParser()\n",
    "\n",
    "def check_if_can_answer_question(state):\n",
    "  result = can_answer_router.invoke({\"question\": state[\"question\"], \"data_description\": DB_DESCRIPTION})\n",
    "\n",
    "  return {\"plan\": result[\"reasoning\"], \"can_answer\": result[\"can_answer\"]}\n",
    "\n",
    "def skip_question(state):\n",
    "  if state[\"can_answer\"]:\n",
    "    return \"no\"\n",
    "  else:\n",
    "    return \"yes\"\n",
    "  \n",
    "write_query_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a database reading bot that can answer users' questions using information from a database. \\n\n",
    "\n",
    "    {data_description} \\n\\n\n",
    "\n",
    "    In the previous step, you have prepared the following plan: {plan}\n",
    "\n",
    "    Return an SQL query with no preamble or explanation. Don't include any markdown characters or quotation marks around the query.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"data_description\", \"question\", \"plan\"],\n",
    ")\n",
    "\n",
    "write_query_chain = write_query_prompt | model | StrOutputParser()\n",
    "\n",
    "def write_query(state):\n",
    "  result = write_query_chain.invoke({\n",
    "      \"data_description\": DB_DESCRIPTION,\n",
    "      \"question\": state[\"question\"],\n",
    "      \"plan\": state[\"plan\"]\n",
    "  })\n",
    "\n",
    "  return {\"sql_query\": result}\n",
    "\n",
    "def execute_query(state):\n",
    "  query = state[\"sql_query\"]\n",
    "\n",
    "  try:\n",
    "    return {\"sql_result\": query_db(query).to_markdown()}\n",
    "  except Exception as e:\n",
    "    return {\"sql_result\", str(e)}\n",
    "  \n",
    "  \n",
    "write_answer_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a database reading bot that can answer users' questions using information from a database. \\n\n",
    "\n",
    "    In the previous step, you have planned the query as follows: {plan},\n",
    "    generated the query {sql_query}\n",
    "    and retrieved the following data:\n",
    "    {sql_result}\n",
    "\n",
    "    Return a text answering the user's question using the provided data.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"plan\", \"sql_query\", \"sql_result\"],\n",
    ")\n",
    "\n",
    "write_answer_chain = write_answer_prompt | model | StrOutputParser()\n",
    "\n",
    "def write_answer(state):\n",
    "  result = write_answer_chain.invoke({\n",
    "      \"question\": state[\"question\"],\n",
    "      \"plan\": state[\"plan\"],\n",
    "      \"sql_result\": state[\"sql_result\"],\n",
    "      \"sql_query\": state[\"sql_query\"]\n",
    "  })\n",
    "\n",
    "  return {\"answer\": result}\n",
    "\n",
    "cannot_answer_prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    You are a database reading bot that can answer users' questions using information from a database. \\n\n",
    "\n",
    "    You cannot answer the user's questions because of the following problem: {problem}.\n",
    "\n",
    "    Explain the issue to the user and apologize for the inconvenience.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \\n\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"problem\"],\n",
    ")\n",
    "\n",
    "cannot_answer_chain = cannot_answer_prompt | model | StrOutputParser()\n",
    "\n",
    "def explain_no_answer(state):\n",
    "  result = cannot_answer_chain.invoke({\n",
    "      \"problem\": state[\"plan\"], # the plan contains an explanation of why we can't answer the question\n",
    "      \"question\": state[\"question\"]\n",
    "  })\n",
    "\n",
    "  return {\"answer\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e6f2e01c4133aeb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T07:49:29.370014Z",
     "start_time": "2024-09-06T07:49:29.365819Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class WorkflowState(TypedDict):\n",
    "  question: str\n",
    "  plan: str\n",
    "  can_answer: bool\n",
    "  sql_query: str\n",
    "  sql_result: str\n",
    "  answer: str\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "\n",
    "workflow = StateGraph(WorkflowState)\n",
    "\n",
    "workflow.add_node(\"check_if_can_answer_question\", check_if_can_answer_question)\n",
    "workflow.add_node(\"write_query\", write_query)\n",
    "workflow.add_node(\"execute_query\", execute_query)\n",
    "workflow.add_node(\"write_answer\", write_answer)\n",
    "workflow.add_node(\"explain_no_answer\", explain_no_answer)\n",
    "\n",
    "workflow.set_entry_point(\"check_if_can_answer_question\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_if_can_answer_question\",\n",
    "    skip_question, # given the text response from this function,\n",
    "    { # we choose which node to go to\n",
    "        \"yes\": \"explain_no_answer\",\n",
    "        \"no\": \"write_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"write_query\", \"execute_query\")\n",
    "workflow.add_edge(\"execute_query\", \"write_answer\")\n",
    "\n",
    "workflow.add_edge(\"explain_no_answer\", END)\n",
    "workflow.add_edge(\"write_answer\", END)\n",
    "\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5035ead5a01a7092",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-06T07:49:30.274172Z",
     "start_time": "2024-09-06T07:49:29.565962Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not StringPromptValue",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount customers by zip code. Return the 5 most common zip codes\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 2\u001b[0m app\u001b[38;5;241m.\u001b[39minvoke(inputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1448\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1447\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1450\u001b[0m     config,\n\u001b[1;32m   1451\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1452\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1453\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39minput_keys,\n\u001b[1;32m   1454\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1455\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1456\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1457\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1458\u001b[0m ):\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1460\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:983\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 983\u001b[0m _panic_or_proceed(done, inflight, step)\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1537\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1535\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1536\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1537\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[1;32m   1542\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langgraph/pregel/retry.py:72\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     70\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:2876\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2874\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m   2875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2876\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2877\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2878\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langgraph/utils.py:95\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[1;32m     94\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m---> 95\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[38], line 26\u001b[0m, in \u001b[0;36mcheck_if_can_answer_question\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_if_can_answer_question\u001b[39m(state):\n\u001b[0;32m---> 26\u001b[0m   result \u001b[38;5;241m=\u001b[39m can_answer_router\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_description\u001b[39m\u001b[38;5;124m\"\u001b[39m: DB_DESCRIPTION})\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplan\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:2878\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2876\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2877\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2878\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   2879\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:4474\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4460\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[1;32m   4461\u001b[0m \n\u001b[1;32m   4462\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4471\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[1;32m   4472\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m   4475\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke,\n\u001b[1;32m   4476\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   4477\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config(config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc),\n\u001b[1;32m   4478\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4479\u001b[0m     )\n\u001b[1;32m   4480\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4483\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4484\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:1785\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1782\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1783\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1784\u001b[0m         Output,\n\u001b[0;32m-> 1785\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m   1786\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m             config,\n\u001b[1;32m   1790\u001b[0m             run_manager,\n\u001b[1;32m   1791\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1792\u001b[0m         ),\n\u001b[1;32m   1793\u001b[0m     )\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1795\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/config.py:398\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    397\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/base.py:4330\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4328\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   4329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4330\u001b[0m     output \u001b[38;5;241m=\u001b[39m call_func_with_variable_args(\n\u001b[1;32m   4331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   4332\u001b[0m     )\n\u001b[1;32m   4333\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[1;32m   4334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/langchain_core/runnables/config.py:398\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    397\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1189\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1186\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1190\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1191\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1192\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1193\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1194\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1195\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1196\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1197\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1198\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1199\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1200\u001b[0m )\n\u001b[1;32m   1202\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:950\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    947\u001b[0m     use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    949\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 950\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(input_ids)\n\u001b[1;32m    952\u001b[0m return_legacy_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    954\u001b[0m     use_cache \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(past_key_values, Cache) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    955\u001b[0m ):  \u001b[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not StringPromptValue"
     ]
    }
   ],
   "source": [
    "inputs = {\"question\": \"Count customers by zip code. Return the 5 most common zip codes\"}\n",
    "app.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab3e64b016a4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a796fb6f32a9d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T09:02:42.214399Z",
     "start_time": "2024-09-05T09:02:41.466621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'weight' and 'mass': 0.45311458655115255\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Define the words\n",
    "word1 = \"weight\"\n",
    "word2 = \"mass\"\n",
    "\n",
    "# Process the words\n",
    "token1 = nlp(word1)\n",
    "token2 = nlp(word2)\n",
    "\n",
    "# Calculate similarity\n",
    "similarity = token1.similarity(token2)\n",
    "print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643d7f82829df03c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-05T09:30:38.577297Z",
     "start_time": "2024-09-05T09:30:38.564337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Levenshtein Distance: 3\n",
      "Levenshtein Similarity Ratio: 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "# Example strings\n",
    "str1 = \"kitten\"\n",
    "str2 = \"sitting\"\n",
    "\n",
    "# Calculate Levenshtein distance\n",
    "distance = Levenshtein.distance(str1, str2)\n",
    "print(f\"Levenshtein Distance: {distance}\")\n",
    "\n",
    "# Calculate similarity ratio\n",
    "similarity = Levenshtein.ratio(str1, str2)\n",
    "print(f\"Levenshtein Similarity Ratio: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae5183204c42db71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T10:53:43.847003Z",
     "start_time": "2024-08-26T10:53:43.837539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'given': [], 'when': [], 'then': []}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def extract_clauses(directory):\n",
    "    clauses = {'given': [], 'when': [], 'then': []}\n",
    "    pattern = re.compile(r'@(given|when|then)\\(\\'(.*?)\\'\\)')\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.py'):\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    content = f.read()\n",
    "                    matches = pattern.findall(content)\n",
    "                    for match in matches:\n",
    "                        clauses[match[0]].append(match[1])\n",
    "    return clauses\n",
    "\n",
    "# Example usage\n",
    "directory = './car-behave-master/features/steps'\n",
    "clauses = extract_clauses(directory)\n",
    "print(clauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6936b9b5d09cfdc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the pattern with named groups\n",
    "pattern = r\"(?P<param1>\\w+)\\s(?P<param2>\\w+)\"\n",
    "\n",
    "# Dictionary with replacement values\n",
    "replacements = {\n",
    "    \"param1\": \"Hello\",\n",
    "    \"param2\": \"World\"\n",
    "}\n",
    "\n",
    "# Function to replace named groups\n",
    "def replace_named_groups(match):\n",
    "    group_dict = match.groupdict()\n",
    "    for key, value in group_dict.items():\n",
    "        if key in replacements:\n",
    "            group_dict[key] = replacements[key]\n",
    "    return \" \".join(group_dict.values())\n",
    "\n",
    "# Text to be processed\n",
    "text = \"foo bar\"\n",
    "\n",
    "# Perform the replacement\n",
    "result = re.sub(pattern, replace_named_groups, text)\n",
    "print(result)  # Output: Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b498fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@given(\"the car has 123 kw, weighs 45 kg, has a drag coefficient of 0.3\")\n"
     ]
    }
   ],
   "source": [
    "# Given a string, find the closing index of a matching parenthesis starting as open_pos\n",
    "def find_matching_parenthesis(s: str, open_pos: int) -> int:\n",
    "    stack = []\n",
    "    for i, char in enumerate(s):\n",
    "        if char == '(':\n",
    "            stack.append(i)\n",
    "        elif char == ')':\n",
    "            if stack:\n",
    "                start = stack.pop()\n",
    "                if start == open_pos:\n",
    "                    return i\n",
    "            else:\n",
    "                return -1 # Incorrect, no matching opening parenthesis\n",
    "    return -1  # Return -1 if no matching parenthesis is found\n",
    "\n",
    "# Example usage\n",
    "gherkin_step = \"@given(\\\"the car has (?P<engine_power>\\\\d+) kw, weighs (?P<weight>\\\\d+) kg, has a drag coefficient of (?P<drag>[\\\\.\\\\d]+)\\\")\"\n",
    "\n",
    "\n",
    "def fill_regex_with_dictvalues(step_regex: str, replacements: dict) -> str:\n",
    "    stack = []\n",
    "    slen = len(step_regex)\n",
    "    \n",
    "    filled_str = \"\" \n",
    "    last_unmatched = 0\n",
    "    \n",
    "    for i, char in enumerate(step_regex):\n",
    "        if char == '(':\n",
    "            remain_sz = slen - i\n",
    "            if remain_sz > 3 and step_regex[i+1] == '?' and step_regex[i+2] == 'P' and step_regex[i+3] == '<':\n",
    "                closing_index = find_matching_parenthesis(step_regex[i:], 0)            \n",
    "                assert closing_index != -1, \"Incorrect, no matching closing parenthesis\"\n",
    "                group = step_regex[i:i+closing_index+1]\n",
    "                \n",
    "                key_to_replace = group[4:group.index('>')]\n",
    "                assert key_to_replace in replacements, f\"Key {key_to_replace} not found in replacements\"\n",
    "                filled_str += step_regex[last_unmatched:i] + replacements[key_to_replace]\n",
    "                \n",
    "                last_unmatched = i+closing_index+1\n",
    "    \n",
    "    filled_str += step_regex[last_unmatched:]\n",
    "    return filled_str\n",
    "                \n",
    "                \n",
    "replacements = {\n",
    "    \"engine_power\": \"123\",\n",
    "    \"weight\": \"45\",\n",
    "    \"drag\": \"0.3\"\n",
    "}\n",
    "print(fill_regex_with_dictvalues(gherkin_step, replacements))  # Output: @given(\"the car has 123 kw, weighs 45 kg, has a drag coefficient of 0.3\")        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f6795455c29369",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T19:30:39.071279Z",
     "start_time": "2024-08-22T19:30:38.978712Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Levenshtein distance between 'kitten' and 'sitting' is 3\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "# Example strings\n",
    "str1 = \"kitten\"\n",
    "str2 = \"sitting\"\n",
    "\n",
    "# Calculate Levenshtein distance\n",
    "distance = Levenshtein.distance(str1, str2)\n",
    "print(f\"The Levenshtein distance between '{str1}' and '{str2}' is {distance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e368d42b-9d9b-4425-adcf-1e9c624cc89f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T07:57:52.847884Z",
     "start_time": "2024-08-21T07:56:36.219990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699d516f3598499c9171d67ec045d9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from typing import List, Dict\n",
    "from pprint import pprint\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a BDD testing expert that can write scenarios using Gherkin language, Python language and behave library\"},\n",
    "    {\"role\": \"user\", \"content\": \"Show me a scenario for testing Mario like games\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "def inference(prompt: str, max_tokens: int ) -> str:\n",
    "    messages[-1][\"content\"] = prompt\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=max_tokens,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        #temperature=0.1,\n",
    "        #top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    return outputs[0][\"generated_text\"][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d76cf6-3efd-4892-97e7-d376da3a21b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcae1db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('{\\n'\n",
      " '  \"found\": true,\\n'\n",
      " '  \"step_found\": \"@given(\\\\\"the car has (?P<engine_power>\\\\\\\\d+) kw, weighs '\n",
      " '(?P<weight>\\\\\\\\d+) kg, has a drag coefficient of '\n",
      " '(?P<drag>[\\\\\\\\.\\\\\\\\d]+)\\\\\")\"\\n'\n",
      " '}')\n"
     ]
    }
   ],
   "source": [
    "USER_INPUT_STEP = \"A drag of 123, a mass of 12345 kg, and an engine of 124kw the Yoda's vehicle has!\"\n",
    "\n",
    "match_rewrite_prompt_template= \"\"\"Given the below Gherkin available steps, check if any is close the input step. If yes, return the closest step in Gherkin syntax, including the @given, @when, or @then before.\n",
    "    \n",
    "    Use Json syntax for response. Use the following format if any step can be matched:\n",
    "    {{\n",
    "      \"found\": true,\n",
    "      \"step_found\":  the step you found closest\n",
    "    }}\n",
    "    \n",
    "    If no available option is OK, then use:\n",
    "    {{\n",
    "        \"found\": false,\n",
    "    }}\n",
    "    \n",
    "    Do not provide any other information or examples.\n",
    "    \n",
    "    ### Input step: \n",
    "    {user_input_step_to_match}\n",
    "    \n",
    "    ### Available steps:\n",
    "    {user_input_available_steps}\"\"\"\n",
    "\n",
    "# TODO: take from file with source code \n",
    "USER_INPUT_AVAILABLE_STEPS = \"\"\"@given(\"the car has (?P<engine_power>\\d+) kw, weighs (?P<weight>\\d+) kg, has a drag coefficient of (?P<drag>[\\.\\d]+)\")\n",
    "    \n",
    "    @given(\"a frontal area of (?P<area>.+) m\\^2\")\n",
    "    \n",
    "    @when(\"I accelerate to (?P<speed>\\d+) km/h\")\n",
    "    \n",
    "    @then(\"the time should be within (?P<precision>[\\d\\.]+)s of (?P<time>[\\d\\.]+)s\")\n",
    "    \n",
    "    @given(\"that the car is moving at (?P<speed>\\d+) m/s\")\n",
    "    \n",
    "    @when(\"I brake at (?P<brake_force>\\d+)% force\")\n",
    "    \n",
    "    @step(\"(?P<seconds>\\d+) seconds? pass(?:es)?\")\n",
    "    \n",
    "    @then(\"I should have traveled less than (?P<distance>\\d+) meters\")\n",
    "    \n",
    "    @given(\"that the car's heading is (?P<heading>\\d+) deg\")\n",
    "    \n",
    "    @when(\"I turn (?P<direction>left|right) at a yaw rate of (?P<rate>\\d+) deg/sec for (?P<duration>\\d+) seconds\")\n",
    "    \n",
    "    @then(\"the car's heading should be (?P<heading>\\d+) deg\")\"\"\"\n",
    "    \n",
    "match_rewrite_prompt = match_rewrite_prompt_template.format(user_input_step_to_match=USER_INPUT_STEP, \n",
    "                                         user_input_available_steps=USER_INPUT_AVAILABLE_STEPS)\n",
    "\n",
    "res0 = inference(match_rewrite_prompt, 1024)[\"content\"]\n",
    "\n",
    "pprint(res0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91ac489c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'{\\n  \"engine_power\": \"124 kw\",\\n  \"weight\": \"12345 kg\",\\n  \"drag\": \"123\"\\n}'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_matching_params_template = \"\"\"Can you match the parameters in the input text step with the target step ?\n",
    "    \n",
    "    Example:\n",
    "    ### Input: The plane has a travel speed of 123 km/h and a lenght of 500 m\n",
    "    ### Target: @given(A plane that has a (?P<speed>\\d+) km/h, length (?P<size>\\d+) m\n",
    "    Response:\n",
    "    {{\n",
    "     \"speed\" : \"123 km/h\",\n",
    "     \"size\" : \"500 m\"\n",
    "    }}\n",
    "    \n",
    "    Your task:\n",
    "    ### Input: {user_input_step}\n",
    "    ### Target: {step_str}\n",
    "    \n",
    "    Response:    your response \n",
    "    \n",
    "    Do not write anything else.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "step_found_str = '@given(\"the car has (?P<engine_power>\\d+) kw, weighs (?P<weight>\\d+) kg, has a drag coefficient of (?P<drag>[\\.\\d]+)\")'\n",
    "prompt_matching_params = prompt_matching_params_template.format(user_input_step=USER_INPUT_STEP, step_str=step_found_str)\n",
    "\n",
    "\n",
    "res = inference(prompt_matching_params, 1024)[\"content\"]\n",
    "pprint(res)\n",
    "#pprint(inference(\"tell me a scenario for testing Mario like games\", 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec7006c0-1b06-4d0a-8a65-d96007b045a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ciprian/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Plain result: {\\n  \"found\": false,\\n}'\n",
      "('exception occured while reading the output: Expecting property name enclosed '\n",
      " 'in double quotes: line 3 column 1 (char 20)')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 135\u001b[0m\n\u001b[1;32m    130\u001b[0m     is_matched, resp \u001b[38;5;241m=\u001b[39m _match_input_step_to_set(\u001b[38;5;28mself\u001b[39m, USER_INPUT_STEP, USER_INPUT_AVAILABLE_STEPS, max_generated_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m is_matched, resp \n\u001b[0;32m--> 135\u001b[0m is_matched, resp \u001b[38;5;241m=\u001b[39m match_input_step_to_set(\u001b[38;5;28;01mNone\u001b[39;00m, USER_INPUT_STEP \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA drag of 123, a mass of 12345 kg, and an engine of 124kw the Yoda\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms vehicle has!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m pprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model matched the step: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mis_matched\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124m            Response: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 130\u001b[0m, in \u001b[0;36mmatch_input_step_to_set\u001b[0;34m(self, USER_INPUT_STEP, max_generated_tokens)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch_input_step_to_set\u001b[39m(\u001b[38;5;28mself\u001b[39m, \n\u001b[1;32m    104\u001b[0m                             USER_INPUT_STEP: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m    105\u001b[0m                             max_generated_tokens : \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    106\u001b[0m     \n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# TODO: take from file with source code \u001b[39;00m\n\u001b[1;32m    108\u001b[0m     USER_INPUT_AVAILABLE_STEPS \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m@given(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe car has (?P<engine_power>\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+) kw, weighs (?P<weight>\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+) kg, has a drag coefficient of (?P<drag>[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md]+)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124m    @given(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma frontal area of (?P<area>.+) m\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m^2\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124m    @then(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe car\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms heading should be (?P<heading>\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+) deg\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 130\u001b[0m     is_matched, resp \u001b[38;5;241m=\u001b[39m _match_input_step_to_set(\u001b[38;5;28mself\u001b[39m, USER_INPUT_STEP, USER_INPUT_AVAILABLE_STEPS, max_generated_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m is_matched, resp\n",
      "Cell \u001b[0;32mIn[2], line 72\u001b[0m, in \u001b[0;36m_match_input_step_to_set\u001b[0;34m(self, USER_INPUT_STEP, USER_INPUT_AVAILABLE_STEPS, max_generated_tokens)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     69\u001b[0m     pprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexception occured while reading the output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m step_found_str \u001b[38;5;241m=\u001b[39m res_json\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep_found\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_found_str \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     prompt_matching_params \u001b[38;5;241m=\u001b[39m prompt_matching_params_template\u001b[38;5;241m.\u001b[39mformat(user_input_step\u001b[38;5;241m=\u001b[39mUSER_INPUT_STEP, step_str\u001b[38;5;241m=\u001b[39mstep_found_str)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "USE_DEBUG = False \n",
    "from typing import Union, Tuple\n",
    "\n",
    "# Takes an input step in natural language and tries to match against a set of available input steps in a Gherkin file.\n",
    "# Two steps are used in the process:\n",
    "    # Step 1: try to find the closest in terms of matching\n",
    "    # Step 2: try to match parameters. Report the error if not succeeded\n",
    "def _match_input_step_to_set(self, \n",
    "                            USER_INPUT_STEP: str, \n",
    "                            USER_INPUT_AVAILABLE_STEPS: str,\n",
    "                            max_generated_tokens : int = 1024) -> Tuple[bool, str]:\n",
    "    \n",
    "    match_rewrite_prompt_template= \"\"\"Given the below Gherkin available steps, check if any can match the input step. \n",
    "    \n",
    "    Use Json syntax for response. Use the following format if any step can be matched:\n",
    "    {{\n",
    "      \"found\": true,\n",
    "      \"step_found\":  the step you found closest\n",
    "    }}\n",
    "    \n",
    "    If no available option is OK, then use:\n",
    "    {{\n",
    "        \"found\": false,\n",
    "    }}\n",
    "    \n",
    "    Do not provide any other information or examples.\n",
    "    \n",
    "    ### Input step: \n",
    "    {user_input_step_to_match}\n",
    "    \n",
    "    ### Available steps:\n",
    "    {user_input_available_steps}\"\"\"\n",
    "    \n",
    "    prompt_matching_params_template = \"\"\"Can you match the parameters in the input text step with the target step ?\n",
    "    \n",
    "    Example:\n",
    "    ### Input: The plane has a travel speed of 123 km/h and a lenght of 500 m\n",
    "    ### Target: @given(A plane that has a (?P<speed>\\d+) km/h, length (?P<size>\\d+) m\n",
    "    Response:\n",
    "    {{\n",
    "     \"speed\" : \"123 km/h\",\n",
    "     \"size\" : \"500 m\"\n",
    "    }}\n",
    "    \n",
    "    Your task:\n",
    "    ### Input: {user_input_step}\n",
    "    ### Target: {step_str}\n",
    "    \n",
    "    Response:    your response \n",
    "    \n",
    "    Do not write anything else.\n",
    "    \"\"\"\n",
    "    \n",
    "    match_rewrite_prompt = match_rewrite_prompt_template.format(user_input_step_to_match=USER_INPUT_STEP, \n",
    "                                         user_input_available_steps=USER_INPUT_AVAILABLE_STEPS)\n",
    "    \n",
    "    import json \n",
    "    res = inference(match_rewrite_prompt, max_generated_tokens)[\"content\"]\n",
    "    res = res.replace(\"\\\\\", \"\\\\\\\\\") # escape the backslashes\n",
    "    pprint(f\"Plain result: {res}\")\n",
    "    \n",
    "    \n",
    "    # Loading in json\n",
    "    res_json = None\n",
    "    try:\n",
    "        dir = json.loads(res)\n",
    "        res_json = dir\n",
    "    except Exception as e:\n",
    "        pprint(f\"exception occured while reading the output: {e}\")\n",
    "    \n",
    "    \n",
    "    step_found_str = res_json.get(\"step_found\", None)\n",
    "    \n",
    "    if step_found_str is not None:\n",
    "        prompt_matching_params = prompt_matching_params_template.format(user_input_step=USER_INPUT_STEP, step_str=step_found_str)\n",
    "        res22 = inference(prompt_matching_params, max_generated_tokens)[\"content\"]\n",
    "        res22 = res22.replace(\"\\\\\", \"\\\\\\\\\") # escape the backslashes\n",
    "        \n",
    "        #RESPONSE_TAG = \"Response:\"\n",
    "        #resp_json_begin_index = res22.find(RESPONSE_TAG) + len(RESPONSE_TAG)\n",
    "        resp_json_begin_index = res22.find(\"{\")\n",
    "        resp_json_end_index = res22.rfind(\"}\")\n",
    "        \n",
    "        if resp_json_begin_index !=-1 and resp_json_end_index!= -1:\n",
    "            resp_json_str = res22[resp_json_begin_index : resp_json_end_index + 1]\n",
    "            \n",
    "            try:\n",
    "                resp_json = json.loads(resp_json_str)\n",
    "                pprint(resp_json)\n",
    "\n",
    "                return (True, resp_json)\n",
    "            except Exception as e:\n",
    "                msg = f\"Error {e} when parsing for parameters:\\n{resp_json_str}\"\n",
    "                pprint(msg)\n",
    "                \n",
    "                return (False, msg)\n",
    "    \n",
    "    \n",
    "    msg = \"The model didn't find any match. The raw output is {temp_resp}\".format(temp_resp=res22)\n",
    "    pprint(msg)\n",
    "    return (False, msg)\n",
    "\n",
    "def match_input_step_to_set(self, \n",
    "                            USER_INPUT_STEP: str, \n",
    "                            max_generated_tokens : int = 1024) -> Tuple[bool, str]:\n",
    "    \n",
    "    # TODO: take from file with source code \n",
    "    USER_INPUT_AVAILABLE_STEPS = \"\"\"@given(\"the car has (?P<engine_power>\\d+) kw, weighs (?P<weight>\\d+) kg, has a drag coefficient of (?P<drag>[\\.\\d]+)\")\n",
    "    \n",
    "    @given(\"a frontal area of (?P<area>.+) m\\^2\")\n",
    "    \n",
    "    @when(\"I accelerate to (?P<speed>\\d+) km/h\")\n",
    "    \n",
    "    @then(\"the time should be within (?P<precision>[\\d\\.]+)s of (?P<time>[\\d\\.]+)s\")\n",
    "    \n",
    "    @given(\"that the car is moving at (?P<speed>\\d+) m/s\")\n",
    "    \n",
    "    @when(\"I brake at (?P<brake_force>\\d+)% force\")\n",
    "    \n",
    "    @step(\"(?P<seconds>\\d+) seconds? pass(?:es)?\")\n",
    "    \n",
    "    @then(\"I should have traveled less than (?P<distance>\\d+) meters\")\n",
    "    \n",
    "    @given(\"that the car's heading is (?P<heading>\\d+) deg\")\n",
    "    \n",
    "    @when(\"I turn (?P<direction>left|right) at a yaw rate of (?P<rate>\\d+) deg/sec for (?P<duration>\\d+) seconds\")\n",
    "    \n",
    "    @then(\"the car's heading should be (?P<heading>\\d+) deg\")\"\"\"\n",
    "    \n",
    "    is_matched, resp = _match_input_step_to_set(self, USER_INPUT_STEP, USER_INPUT_AVAILABLE_STEPS, max_generated_tokens=1024)\n",
    "\n",
    "    return is_matched, resp \n",
    "\n",
    "\n",
    "is_matched, resp = match_input_step_to_set(None, USER_INPUT_STEP = \"A drag of 123, a mass of 12345 kg, and an engine of 124kw the Yoda's vehicle has!\")\n",
    "pprint(f\"The model matched the step: {is_matched}\\n \\\n",
    "            Response: {resp}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c425b3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'found': True,\n",
       " 'step_found': 'the car has (?P<engine_power>\\\\d+) kw, weighs (?P<weight>\\\\d+) kg, has a drag coefficient of (?P<drag>[\\\\.\\\\d]+)'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = '{\\n  \"found\": true,\\n  \"step_found\":  \"the car has (?P<engine_power>\\\\d+) kw, weighs (?P<weight>\\\\d+) kg, has a drag coefficient of (?P<drag>[\\\\.\\\\d]+)\"\\n}'\n",
    "\n",
    "inp = inp.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "\n",
    "#r = inp[66:]\n",
    "#print(r)\n",
    "\n",
    "import json\n",
    "json.loads(inp, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424127768c801f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
